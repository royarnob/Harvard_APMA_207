{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #1 (Due 09/17/2020, 11:59pm)\n",
    "## Maximum Likelihood Learning and Bayesian Inference\n",
    "\n",
    "**AM 207: Advanced Scientific Computing**<br>\n",
    "**Instructor: Weiwei Pan**<br>\n",
    "**Fall 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Arnob Roy\n",
    "\n",
    "**Students collaborators:** I spoke with Ansuman Praty (I hope that is how you spell his name) mostly about part 2. He mentioned that he had already submitted his work by the time I asked him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "**Submission Format:** Use this notebook as a template to complete your homework. Please intersperse text blocks (using Markdown cells) amongst `python` code and results -- format your submission for maximum readability. Your assignments will be graded for correctness as well as clarity of exposition and presentation -- a “right” answer by itself without an explanation or is presented with a difficult to follow format will receive no credit.\n",
    "\n",
    "**Code Check:** Before submitting, you must do a \"Restart and Run All\" under \"Kernel\" in the Jupyter or colab menu. Portions of your submission that contains syntactic or run-time errors will not be graded.\n",
    "\n",
    "**Libraries and packages:** Unless a problems specifically asks you to implement from scratch, you are welcomed to use any `python` library package in the standard Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import scipy as sp\n",
    "from scipy.stats import mode\n",
    "from scipy.stats import dirichlet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta_prior(a, b, color, ax):\n",
    "    '''--------  plot_beta_prior\n",
    "    A function to visualize a beta pdf on a set of axes\n",
    "    Input: \n",
    "         a (parameter controlling shape of beta prior)\n",
    "         b (parameter controlling shape of beta prior)\n",
    "         color (color of beta pdf)\n",
    "         ax (axes on which to plot pdf)\n",
    "    Returns: \n",
    "         ax (axes with plot of beta pdf)\n",
    "    '''\n",
    "    \n",
    "    #Create a beta-distributed random variable with shape a, b\n",
    "    rv = sp.stats.beta(a, b)\n",
    "    #Create values from 0 to 1\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    #Plot the beta pdf for values from 0 to 1\n",
    "    ax.plot(x, rv.pdf(x), '-', lw=2, color=color, label='a=' + str(a) + ', b=' + str(b))\n",
    "    #Set title, legend etc\n",
    "    ax.set_title('Beta prior with a=' + str(a) + ', b=' + str(b))\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "## CREDIT: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.dirichlet.html\n",
    "def plot_dir_prior(a, b, color, ax):\n",
    "\n",
    "    rv = sp.stats.dirichlet(a, b)\n",
    "   \n",
    "    x = np.linspace(0, 1, 100)\n",
    "    \n",
    "    ax.plot(x, rv.pdf(x), '-', lw=2, color=color, label='a=' + str(a) + ', b=' + str(b))\n",
    "   \n",
    "    ax.set_title('Beta prior with a=' + str(a) + ', b=' + str(b))\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def sample_posterior(a, b, likes, ratings, n_samples):\n",
    "    '''--------  sample_posterior\n",
    "    A function that samples points from the posterior over a movie's \n",
    "    likability, given a binomial likelihood function and beta prior\n",
    "    Input: \n",
    "         a (parameter controlling shape of beta prior)\n",
    "         b (parameter controlling shape of beta prior)\n",
    "         likes (the number of likes in likelihood)\n",
    "         ratings (total number of ratings in likelihood)\n",
    "         n_samples (number of samples to take from posterior)\n",
    "    Returns: \n",
    "         post_samples (a array of points from the posterior)\n",
    "    '''\n",
    "    #Samples points from a beta distribution \n",
    "    #(the posterior of a binomial likelihood and a beta prior is a beta distribution!)\n",
    "    post_samples = np.random.beta(a + likes, b + ratings - likes, n_samples)\n",
    "    return post_samples\n",
    "\n",
    "## CREDIT: https://numpy.org/doc/stable/reference/random/generated/numpy.random.dirichlet.html\n",
    "def sample_dir_posterior(a, n_samples):\n",
    "\n",
    "    post_samples = np.random.dirichlet(a, n_samples)\n",
    "    \n",
    "    return post_samples\n",
    "\n",
    "\n",
    "def find_mode(values, num_bins):\n",
    "    '''--------  find_mode\n",
    "    A function that approximates the mode of a distribution given a sample from the distribution\n",
    "    Input: \n",
    "         values (samples from the distribution)\n",
    "         num_bins (number of bins to use in approximating histogram)\n",
    "    Returns: \n",
    "         mode (the approximate mode of the distribution)\n",
    "    '''\n",
    "    \n",
    "    #Make an approximation (histogram) of the distribution using the samples\n",
    "    bins, edges = np.histogram(values, bins=num_bins)\n",
    "    #Find the bin in the histogram with the max height\n",
    "    max_height_index = np.argmax(bins)\n",
    "    #Find the sample corresponding to the bin with the max height (the mode)\n",
    "    mode = (edges[max_height_index] + edges[max_height_index + 1]) / 2.\n",
    "    \n",
    "    return mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "In the competitive rubber chicken retail market, the success of a company is built on satisfying the exacting standards of a consumer base with refined and discriminating taste. In particular, customer product reviews are all important. But how should we judge the quality of a product based on customer reviews?\n",
    "\n",
    "On Amazon, the first customer review statistic displayed for a product is the ***average rating***. The following are the main product pages for two competing rubber chicken products, manufactured by Lotus World and Toysmith respectively:\n",
    "\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](lotus1.png) |  ![alt](toysmith1.png)\n",
    "\n",
    "Clicking on the 'customer review' link on the product pages takes us to a detailed break-down of the reviews. In particular, we can now see the number of times a product is rated a given rating (between 1 and 5 stars).\n",
    "\n",
    "Lotus World |  Toysmith\n",
    "- |  - \n",
    "![alt](lotus2.png) |  ![alt](toysmith2.png)\n",
    "\n",
    "\n",
    "In the following, we will ask you to build statistical models to compare these two products using the observed rating. Larger versions of the images are available in the data set accompanying this notebook.\n",
    "\n",
    "\n",
    "\n",
    "## Part I: A Maximum Likelihood Model\n",
    "1. **(Model Building)** Suppose that for each product, we can model the probability of the value each new rating as the following vector:\n",
    "$$\n",
    "\\theta = [\\theta_1, \\theta_2, \\theta_3, \\theta_4, \\theta_5]\n",
    "$$\n",
    "  where $\\theta_i$ is the probability that a given customer will give the product $i$ number of stars. That is, each new rating (a value between 1 and 5) has a categorical distribution $Cat(\\theta)$. Represent the observed ratings of an Amazon product as a vector $R = [r_1, r_2, r_3, r_4, r_5]$ where, for example, $r_4$ is the number of $4$-star reviews out of a total of $N$ ratings. Write down the likelihood of $R$. That is, what is $p(R| \\theta)$?\n",
    "\n",
    "  **Note:** The observed ratings for each product should be read off the image files included in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***My Answer Attempt.***\n",
    "\n",
    "Let the outcome be modeled by,\n",
    "$$\n",
    "Y \\sim Cat(\\theta) : \\theta=[\\theta_1, \\theta_2,\\dots,\\theta_5] \\to \\sum^5_{i=1} \\theta_i = 1: \\mathbb{P}(i)=\\theta_i\n",
    "$$\n",
    "\n",
    "From the \"Die Roll\" slide of the lecture 3 notebook, it seems as though this example best follows the multinomial model where there is a $N$ number of independent and identically distributed outcomes $R_1,\\dots R_n \\sim p(R|\\theta)$ observations of Amazon product ratings represented as vector $R=[r_1,r_2,r_3,r_4,r_5$.\n",
    "\n",
    "The probability mass function of a multinomial distribution can be defined as,\n",
    "$$\n",
    "p(R_1=r_1, \\dots, R_5=r_5)=\\dfrac{n!}{r_1!\\cdots r_5!}p_1^{r_1}\\cdots p_5^{r_5}: n\\in\\mathbb{N}\n",
    "$$\n",
    "Credit: https://en.wikipedia.org/wiki/Multinomial_distribution\n",
    "\n",
    "Thus, the likelihood function of $R$ can be written as,\n",
    "$$\n",
    "\\mathcal{L}(\\theta)=\\prod^N_{n=1} p(R|\\theta)=\\prod^N_{n=1} \\dfrac{n!}{r_1!\\cdots r_5!}p_1^{r_1}\\cdots p_5^{r_5}\n",
    "$$\n",
    "\n",
    "For the Loftus World rubber chicken ratings, $N=162$ and $R=[r_1=(6\\%*162),r_2=(4\\%*162),r_3=(6\\%*162),r_4=(17\\%*162),r_5=(67\\%*162)]$\n",
    "$$\n",
    "\\to R=[9.72, 6.48, 9.72, 27.54, 108.54]\n",
    "$$\n",
    "\n",
    "Then the likelihood function for the Loftus World ratings can be written as,\n",
    "$$\n",
    "\\mathcal{L}(\\theta)=\\prod^N_{n=1} p(R|\\theta)=\\prod^N_{n=1} \\dfrac{162!}{9.72!\\ 6.48!\\ 9.72!\\ 27.54!\\ 108.54!}p_1^{9.72!}\\cdots p_5^{108.54!}\n",
    "$$\n",
    "\n",
    "For the Toysmith rubber chicken ratings, $N=410$ and $R=[r_1=(14\\%*410), r_2=(8\\%*410), r_3=(7\\%*410), r_4=(11\\%*410), r_5=(60\\%*410)]$\n",
    "$$\n",
    "\\to R=[57.40, 32.8, 28.7, 45.1, 246.0]\n",
    "$$\n",
    "\n",
    "Then the likelihood function for the Toysmith World ratings can be written as,\n",
    "$$\n",
    "\\mathcal{L}(\\theta)=\\prod^N_{n=1} p(R|\\theta)=\\prod^N_{n=1} \\dfrac{410!}{57.4!\\ 32.8!\\ 28.7!\\ 45.1!\\ 246!}p_1^{57.40!}\\cdots p_5^{246!}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.719999999999999, 6.48, 9.719999999999999, 27.540000000000003, 108.54]\n",
      "[57.400000000000006, 32.8, 28.700000000000003, 45.1, 246.0]\n"
     ]
    }
   ],
   "source": [
    "# Some simple calculation\n",
    "\n",
    "#Loftus World, R Vector\n",
    "loftus = [0.06*162, 0.04*162, 0.06*162, 0.17*162, 0.67*162]\n",
    "\n",
    "#Toysmith, R Vector\n",
    "toysmith = [0.14*410, 0.08*410, 0.07*410, 0.11*410, 0.6*410]\n",
    "\n",
    "print(loftus)\n",
    "print(toysmith)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Model Fitting)** Find the maximum likelihood estimator of $\\theta$ for the Lotus World model; find the MLE of $\\theta$ for the Toysmith model. You need to make a reasonably mathematical argument for why your estimate actually maximizes the likelihood (i.e. recall the criteria for a point to be a global optima of a function).\n",
    "\n",
    "  *Note:* I recommend deriving the MLE using the general expression of the likelihood. That is, derive the posterior using the variable $R$, then afterwards plug in your specific values of $R$ for each product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general expression of my likelihood was,\n",
    "$$\n",
    "\\mathcal{L}(\\theta)=\\prod^N_{n=1} p(R|\\theta)=\\prod^N_{n=1} \\dfrac{n!}{r_1!\\cdots r_5!}p_1^{r_1}\\cdots p_5^{r_5}\n",
    "$$\n",
    "\n",
    "Re-write more general form,\n",
    "$$\n",
    "\\mathcal{L}(\\theta)=\\prod^N_{n=1} \\dfrac{n!(p_i^{r_i)}}{r_i!}\n",
    "$$\n",
    "credit: https://blog.jakuba.net/maximum-likelihood-for-multinomial-distribution/\n",
    "\n",
    "To find MLE, we take the derivative of the likelihood function with respect to its parameters and find its critical points. Here $\\theta$ represents the true probability of each type of rating $i$ (i.e. 1-star, 2-star...). Thus, I am most interested in finding $p_1^{MLE}, p_2^{MLE},...,p_5^{MLE}.$\n",
    "\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "p_1^{MLE}, p_2^{MLE},...,p_5^{MLE}=\\arg\\max_{p_1, p_2, \\dots, p_5} n! \\prod^N_{n=1} \\dfrac{p_1^{r_1}\\cdots p_5^{r_5}}{r_1!\\cdots r_5!}\n",
    "$$\n",
    "\n",
    "But first, consider log-likelihood since this is a monotonically increasing multiplicative function,\n",
    "$$\n",
    "\\log \\mathcal{L}(\\theta)=\\log n! \\prod^N_{n=1} \\dfrac{p_i^{r_i}}{r_i!}\n",
    "$$\n",
    "\n",
    "Which simplifies to,\n",
    "$$\n",
    "\\log \\mathcal{L}(\\theta)=\\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i!\n",
    "$$\n",
    "\n",
    "This optimization problem has the constraint,\n",
    "$$\n",
    "\\sum^N_{n=1}p_i=1\n",
    "$$\n",
    "\n",
    "Credit to Jakub Arnold (linked above) who derived the MLE for the multinomial distribution in his blog. He differentiated the lagrangian with respect to the true probability $p_i$ (in his blog it was $\\pi_i$) and with the previously mentioned constraint,\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial p_i} \\mathcal{L} (p, \\lambda)= \\dfrac{\\partial}{\\partial p_i} \\mathcal{L} (p) + \\dfrac{\\partial}{\\partial p_i} \\mathcal{L} (\\lambda) (1-\\sum^N_{n=1}p_i) \\longrightarrow p_i=\\dfrac{r_i}{\\lambda}\n",
    "$$\n",
    "\n",
    "Then he solved for $\\lambda$ using the given constraint\n",
    "\n",
    "$$\n",
    "p_i=\\dfrac{r_i}{\\lambda} \\longrightarrow p_i=\\dfrac{r_i}{n}\n",
    "$$\n",
    "\n",
    "A reasonably mathematical argument for why this estimate would maximize the likelihood can be shown using definition 4.2 of \"Convex Optimization Overview\" (Kolter). The definition states that $\\text{a point }x\\text{ is globally optimal if it is feasible and }\\forall\\text{ feasible points }z, f(x)\\le f(z).$ Now, if this is a convex optimization problem, all locally optimal points are globally optimal. (Page 8, Kolter)\n",
    "\n",
    "So the MLE for Loftus World for each $p_i$:\n",
    "$$\n",
    "p_1=\\frac{9.72}{162}=0.06,\\ p_2=\\frac{6.48}{162}=0.04,\\ p_3=\\frac{9.72}{162}=0.06,\\ p_4=\\frac{27.54}{162}=0.17,\\ p_5=\\frac{108.54}{162}=0.67\n",
    "$$\n",
    "\n",
    "And the MLE for Toysmith for each $p_i$:\n",
    "$$\n",
    "p_1=\\frac{57.4}{410}=0.14,\\ p_2=\\frac{32.8}{410}=0.08,\\ p_3=\\frac{28.7}{410}=0.07,\\ p_4=\\frac{45.1}{410}=0.11,\\ p_5=\\frac{246.0}{410}=0.6\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.060000000000000005, 0.04, 0.060000000000000005, 0.16999999999999998, 0.67]\n",
      "[0.13999999999999999, 0.07999999999999999, 0.06999999999999999, 0.11, 0.6]\n"
     ]
    }
   ],
   "source": [
    "loftus_mle = [9.72/162, 6.48/162, 9.72/162, 27.54/162, 108.54/162]\n",
    "toysmith_mle = [57.4/410, 32.8/410, 28.7/410, 45.1/410, 246/410]\n",
    "\n",
    "print(loftus_mle)\n",
    "print(toysmith_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Model Interpretation)** Based on your MLE of $\\theta$'s for both models, do you feel confident deciding if one product is superior to another? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am still unsure if MLE has really given me much more information than I had observed from the ratings page.\n",
    "\n",
    "I am not sure how consistent or biased the MLE is for the ratings, so I am not much more confident than I was before.\n",
    "\n",
    "While there are many more ratings for the Toysmith ($N=410$), MLE tells me that the true probability of a 5-star rating is 60% and 4-star rating is 11%. While there are less rating for Loftus World ($N=162$) MLE tells me that the true probability of a 5-star rating is 67% and 4-star rating is 17%.\n",
    "\n",
    "It seems that the Loftus World rating is \"higher\" but is this rating biased? Is the rating higher because it was a less expensive rubber chicken (before shipping, and still slightly less after shipping) and customers expected less? The Toysmith rubber chicken has Amazon Prime shipping, does this alter customer's expectations. In reality, there may be many confounding variables that we are unaware of, and given the limited information provided for the vector $R$, we have not \"sampled\" the ratings of many different \"samples of customers\" to get the best possible answer to our question here. Thus, I cannot fully determine if one product is \"superior\" to another, simply based on average customer ratings.\n",
    "\n",
    "There may be other methods of analyzing the ratings to better account of customer expectations which may help us decide in the future. We have also not checked for outliers, as MLE can be sensitive to outliers; i.e. a customer giving 1-star due to bad shipping service which is unrelated to the product, or giving a 5-star review because he or she is associated with the business selling the rubber chickens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: A Bayesian Model\n",
    "\n",
    "1. **(Model Building)** Suppose you are told that customer opinions are very polarized in the retail world of rubber chickens, that is, most reviews will be 5 stars or 1 stars (with little middle ground). What would be an appropriate $\\alpha$ for the Dirichlet prior on $\\theta$? Recall that the Dirichlet pdf is given by:\n",
    "$$\n",
    "p_{\\Theta}(\\theta) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}, \\quad B(\\alpha) = \\frac{\\prod_{i=1}^k\\Gamma(\\alpha_i)}{\\Gamma\\left(\\sum_{i=1}^k\\alpha_i\\right)},\n",
    "$$\n",
    "where $\\theta_i \\in (0, 1)$ and $\\sum_{i=1}^k \\theta_i = 1$, $\\alpha_i > 0 $ for $i = 1, \\ldots, k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: https://en.wikipedia.org/wiki/Dirichlet_distribution\n",
    "\n",
    "Useful fact: The Dirichlet distribution is conjugate prior to the categorical/multinomial distribution\n",
    "\n",
    "An appropriate $\\alpha$ could possibly be $\\alpha=0.5$. I say this because in the Beta-distribution an $\\alpha=\\beta=0.5$ gives a distribution where there is \"little middle ground,\" and the Dirichlet distribution can be thought of as a multivariate beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "\n",
    "ax_ind = 0\n",
    "\n",
    "plot_beta_prior(0.5, 0.5, 'blue', ax[ax_ind])\n",
    "\n",
    "plt.tight_layout()    \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Inference)** Analytically derive the posterior distribution (using the likelihoods you derived in Part I) for each product.\n",
    "\n",
    "  *Note:* I recommend deriving the posterior using the general expression of a Dirichelet pdf. That is, derive the posterior using the variable $\\alpha$, then afterwards plug in your specific values of $\\alpha$ when you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "\\begin{aligned}\n",
    "p(\\theta | Y) = \\frac{\\overbrace{p(Y| \\theta)}^{\\text{likelihood}} \\overbrace{p(\\theta)}^{\\text{prior}}}{\\underbrace{p(Y)}_{\\text{marginal data likelihood}}} = \\frac{p(Y, \\theta)}{\\int p(Y, \\theta) d\\theta}\n",
    "\\end{aligned}\n",
    "\n",
    "Then, from part I:\n",
    "\n",
    "$$\n",
    "\\longrightarrow \\dfrac{\\mathcal{L}(\\theta)p_\\Theta(\\theta)}{p(Y)}=  \\dfrac{\\prod^N_{n=1} \\dfrac{n!(p_i^{r_i})}{r_i!} \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}}{p(Y)}\n",
    "\\longrightarrow \\dfrac{\\prod^N_{n=1} \\dfrac{n!(p_i^{r_i})}{r_i!} \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}}{\\int \\prod^N_{n=1} \\dfrac{n!(p_i^{r_i})}{r_i!} \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1} d\\theta}\n",
    "$$\n",
    "\n",
    "Since $P(Y, \\theta)$ is monotonically increasing, consider taking the logarithm,\n",
    "\n",
    "$$\n",
    "\\dfrac{\\log(\\prod^N_{n=1} \\dfrac{n!(p_i^{r_i})}{r_i!} \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1})}{\\int \\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! \\log (\\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}) d\\theta} \\longrightarrow \\dfrac{\\log(\\prod^N_{n=1} \\dfrac{n!(p_i^{r_i})}{r_i!} \\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1})}{\\int \\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! \\log (\\frac{1}{B(\\alpha)} \\prod_{i=1}^k \\theta_i^{\\alpha_i - 1}) d\\theta} \n",
    "$$\n",
    "\n",
    "Finish taking the logarithm and plug in the paramaters for $\\theta_i$,\n",
    "\n",
    "$$\n",
    "\\longrightarrow \\dfrac{\\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! + \\log (\\frac{1}{B(\\alpha)}) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i))}{\\int \\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! + \\log (\\frac{1}{B(\\alpha)}) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i)) d\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\longrightarrow \\dfrac{\\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! + \\log (1)-\\log B(\\alpha) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i))}{\\int \\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! + \\log (1)-\\log B(\\alpha) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i)) d\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\longrightarrow \\dfrac{\\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! + 0 -\\log B(\\alpha) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i))}{\\int \\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! + 0 -\\log B(\\alpha) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i)) d\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(The Maximum A Posterior Estimate)** Analytically or empirically compute the MAP estimate of $\\theta$ for each product, using the $\\alpha$'s you chose in Problem 1. How do these estimates compare with the MLE? Just for this problem, compute the MAP estimate of $\\theta$ for each product using a Dirichelet prior with hyperparameters $\\alpha = [1, 1, 1, 1, 1]$. Make a conjecture about the effect of the prior on the difference between the MAP estimates and the MLE's of $\\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My code does not seem to work despite renaming the functions from the in-class exercise 3 to the respective Dirichlet ones from SciPy and NumPy.\n",
    "\n",
    "Analytically,\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{MAP}} = \\mathrm{argmax}_{\\theta} p(\\theta|Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\dfrac{\\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! -\\log B(\\alpha) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i))}{\\int \\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! -\\log B(\\alpha) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i)) d\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "color kwarg must have one color per data set. 5 data sets and 1 colors were provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-93185ca21e01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Approximate the posterior with a histogram of these samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0max_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Find the approximate mode of the posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   6614\u001b[0m                     \u001b[0;34m\"color kwarg must have one color per data set. %d data \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6615\u001b[0m                     \"sets and %d colors were provided\" % (nx, len(color)))\n\u001b[0;32m-> 6616\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6618\u001b[0m         \u001b[0mhist_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: color kwarg must have one color per data set. 5 data sets and 1 colors were provided"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "param = [1,2,3,4,5]\n",
    "\n",
    "# Number of samples to use when approximating our posterior\n",
    "n_samples = 10000\n",
    "\n",
    "# Plot the posterior corresponding to each prior\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 4))\n",
    "\n",
    "# Draw samples from the posterior corresponding to a particular beta prior\n",
    "post_samples = np.random.dirichlet(param, 10000)\n",
    "\n",
    "# Approximate the posterior with a histogram of these samples\n",
    "ax[ax_ind].hist(post_samples, bins=30, color='red', alpha=0.5)\n",
    "\n",
    "# Find the approximate mode of the posterior\n",
    "mode = find_mode(post_samples, 30)\n",
    "\n",
    "# Plot the mode as a vertical line\n",
    "ax[ax_ind].axvline(x=mode, linewidth=3, label='Posterior mode')\n",
    "    \n",
    "# Set title, legends etc\n",
    "ax[ax_ind].set_title('Posterior, with Beta prior (a={}, b={})'.format(a, b))\n",
    "ax[ax_ind].legend(loc='best')\n",
    "\n",
    "# Increment the subplot index\n",
    "ax_ind += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **(The Posterior Mean Estimate)** Analytically or empirically compute the posterior mean estimate of $\\theta$ for each product, using the $\\alpha$'s you chose in Problem 1. How do these estimates compare with the MAP estimates and the MLE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since my code is not working, analytically,\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{post mean}} = \\mathbb{E}_{\\theta\\sim p(\\theta|Y)}\\left[ \\theta|Y \\right] = \\int \\theta p(\\theta|Y) d\\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\longrightarrow \\int \\theta \\dfrac{\\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! -\\log B(\\alpha) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i))}{\\int \\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! -\\log B(\\alpha) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i)) d\\theta} d\\theta\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **(The Posterior Predictive Estimate)** Sample 1000 rating vectors from the posterior predictive for each product, using the $\\alpha$'s you chose in Problem 1. Use the average of the posterior predictive samples to estimate $\\theta$. How do these estimates compare with the MAP, MLE, posterior mean estimate of $\\theta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since code not working, analytically,\n",
    "\n",
    "$$\n",
    "p(Y^*|Y) = \\int_\\Theta p(Y^*, \\theta|Y) d\\theta = \\int_\\Theta p(Y^* | \\theta) p(\\theta | Y) d\\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\int_\\Theta p(Y^* | \\theta) \\dfrac{\\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! -\\log B(\\alpha) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i))}{\\int \\log n! + \\sum ^N_{n=1} r_i \\log p_i -  \\sum ^N_{n=1} r_i! -\\log B(\\alpha) + \\log \\sum_{i=1}^k (\\alpha_i - 1)(p_i)) d\\theta} d\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter vector 'a' must be one dimensional, but a.shape = ().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7783d86b686c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-7783d86b686c>\u001b[0m in \u001b[0;36mmake_models\u001b[0;34m(q, alpha, N, theta)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#prior definition: beta distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#sample data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, alpha, seed)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdirichlet_frozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_logpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, alpha, seed)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mdirichlet_frozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_rv_frozen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dirichlet_check_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m_dirichlet_check_parameters\u001b[0;34m(alpha)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All parameters must be greater than 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m         raise ValueError(\"Parameter vector 'a' must be one dimensional, \"\n\u001b[0m\u001b[1;32m   1244\u001b[0m                          \"but a.shape = %s.\" % (alpha.shape, ))\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter vector 'a' must be one dimensional, but a.shape = ()."
     ]
    }
   ],
   "source": [
    "## ATTEMPT 2 USING LECTURE PYTHON CODE\n",
    "\n",
    "#a function for computing the prior and the posterior (after simulating data)\n",
    "def make_models(q, alpha, N, theta):\n",
    "    #prior definition: beta distribution\n",
    "    prior = sp.stats.dirichlet(q, alpha).pdf\n",
    "    \n",
    "    #sample data\n",
    "    H = np.random.dirichlet(N, theta)\n",
    "    \n",
    "    #update posterior: beta distribution\n",
    "    posterior_alpha = H + alpha\n",
    "    posterior_beta = N - H + beta\n",
    "    posterior = sp.stats.beta(posterior_alpha, posterior_beta).pdf\n",
    "    \n",
    "    return prior, posterior, H, posterior_alpha, posterior_beta\n",
    "\n",
    "# a function for ploting the prior and posterior distribution\n",
    "def plot_prior_posterior(ax, prior, posterior, H, N):\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    \n",
    "    ax[0].plot(x, prior(x), color='blue')\n",
    "    ax[0].set_title('Prior distribution')\n",
    "    ax[1].plot(x, posterior(x), color='red')\n",
    "    ax[1].set_title('Posterior distribution after seeing {} Heads/{} Flips'.format(H, N))\n",
    "    return ax\n",
    "\n",
    "#prior definition: beta distribution\n",
    "#try: alpha = beta = 0.5, 1, 10; try: alpha=10, beta=1; try: alpha=1, beta=10\n",
    "alpha = 1\n",
    "q = [2.5, 97.5]\n",
    "beta = 0.5\n",
    "#data: binomial distribution\n",
    "N = 10\n",
    "theta = 0.3\n",
    "\n",
    "prior, posterior, H, posterior_alpha, posterior_beta = make_models(1, alpha, N, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **(Model Evaluation)** Compute the 95% credible interval of $\\theta$ for each product (*Hint: compute the 95% credible interval for each $\\theta_i$, $i=1, \\ldots, 5$*). For which product is the posterior mean and MAP estimate more reliable and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would assume that the maximum a posterior estimate was more reliable since it took into account our Dirichlet prior. I wish my code was working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Comparison\n",
    "1. **(Summarizing Customer Ratings)** Recall that on Amazon, the first customer review statistic displayed for a product is the average rating. Name at least one problem with ranking products based on the average customer rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An immediate comment on the \"average customer rating\" I can make is that we do not have all of the information regarding what matters to the customer, and what types of outliers there may be. Estimation such as MLE is quite sensitive to outliers and weakens the realiability of the estimates it spits out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Comparison of Point Estimates)** Which point estimate (MAP, MLE, posterior mean or posterior predictive estimate) of $\\theta$, if any, would you feel choose to rank the two Amazon products? Why? \n",
    "\n",
    "  *Hint: think about which of these estimates are equivalent (if any). If they are not equivalent, what are the special properties of each estimate? What aspect of the data or the model is each estimate good at capturing?*\n",
    "  \n",
    "   **Note:** we're not looking for \"the correct answer\" here. We are looking for a sound decision based on a statistically correct interpretation of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the four estimation methods, I would suggest that MLE has some weaknesses. Particularly with since we did not verify that it is unbiased and consistent, and that there were no outliers. I would suggest that when we considered a prior belief where there was \"no middle ground,\" we thought customers either loved or hated the product, thus we were able to create better estimators based on this prior belief. I would say that the estimators incorporating Bayesian Analysis were more resilient to MLE's weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
